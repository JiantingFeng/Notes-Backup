- Link: https://arxiv.org/pdf/2106.09685.pdf
- Main Idea: Assume that in each gradient step, only a few number of parameters are updated. (According to the original paper, for GPT-3, this number can be only $$0.01\%$$)
	- In each step, suppose our target is $$\max_{\theta\in\Theta}\sum_{(x, y)\in \mathcal{Z}}\log (y\vert x)$$, which could be extended into other models, i.e. autoregressive.
	- Suppose we freeze most of the parameters, i.e., only a subset $$\theta_0$$ can be updated
	- $$\max_{\Delta\theta}\sum_{(x, y)\in \mathcal{Z}}\log (y\vert x)$$