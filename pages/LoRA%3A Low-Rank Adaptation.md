- Link: https://arxiv.org/pdf/2106.09685.pdf
- Main Idea: Assume that in each gradient step, only a few number of parameters are updated. (According to the original paper, for GPT-3, this number can be only $$0.01\%$$)
	- In each step, suppose our target is $$\max_{\theta\in\Theta}\sum_{(x, y)\in \mathcal{Z}}\log P_{\theta}( y\vert x)$$, which could be extended into other models, i.e. autoregressive.
	- Suppose we freeze most of the parameters, i.e., only a subset $$\Delta\theta$$ can be updated, where $$\Delta \theta$$ is low rank
	- $$\max_{\Delta\theta}\sum_{(x, y)\in \mathcal{Z}}\log P_{\theta+\Delta\theta}( y\vert x)$$
- For each dense layer, $$h = (W + \Delta W) x$$
-