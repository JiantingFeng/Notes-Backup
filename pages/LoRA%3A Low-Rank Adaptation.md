- Link: https://arxiv.org/pdf/2106.09685.pdf
-
- When we need to fine tune a pretrained model, we donnot have to modify all the parameters, in stead, we can only train a few of them.
- Main Idea: Assume that in each gradient step, only a few number of parameters are updated. (According to the original paper, for GPT-3, this number can be only $$0.01\%$$)
	- In each step, suppose our target is $$\max_{\theta\in\Theta}\sum_{(x, y)\in \mathcal{Z}}\log P_{\theta}( y\vert x)$$, which could be extended into other models, i.e. autoregressive.
	- Suppose we freeze most of the parameters, i.e., only a subset $$\Delta\theta$$ can be updated, where $$\Delta \theta$$ is low rank
	- $$\max_{\Delta\theta}\sum_{(x, y)\in \mathcal{Z}}\log P_{\theta+\Delta\theta}( y\vert x)$$
- For each dense layer, in every gradient step $$h = (W + \Delta W) x$$
- Suppose $$\Delta W = AB$$, where $$A \in\mathbb{R}^{m\times r}$$ and $$B \in\mathbb{R}^{r\times n}$$, matrix $$A$$ with full column rank and $$B$$ with full row rank.
- We only train $$\Delta W$$, the number of parameter is $$mr+rn \ll mn $$ when $$r$$ is small