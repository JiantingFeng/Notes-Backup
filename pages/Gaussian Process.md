- Gaussian process is equivalent to the bayesian learning with exact 1 infinite width hidden layer.
- Idea: **prior distribution over infinite dimensional functions**.
- Gaussian Process Regression
	- Gaussian distribution on one single point: $$P(y_i\vert x_i; w)=\mathcal{N}(w^Tx, \sigma^2 I)$$
	- MLE: $$\displaystyle P(\mathcal{D}\vert w) = \prod_{i=1}^n P(y_i\vert x_i; w)$$
	- MAP: $$P(w\vert \mathcal D) = \dfrac{P(\mathcal{D}\vert w) P(w)}{Z}$$
	- Directly model the distribution of $$y$$ (w/o learning the model)
		- $$P(y\vert x, \mathcal D) = \int_w \underbrace{P(y\vert x, w, \mathcal{D})}_{\text{Gaussian}} \underbrace{P(w\vert \mathcal D)}_{\text{Gaussian prior}}dw$$
		- Therefore, $$P(y\vert x, \mathcal D)$$ is still Gaussian, we don't need to calculate the exact form, because we know how Gaussian looks like.
		- $$P(y\vert x, \mathcal D) \sim \mathcal{N}(\mu, \Sigma)$$
- Definition [Gaussian Process]
	- A GP is a (potentially infinite) collection of random variables such that the joint distribution of every finite subset of RVs is multivariate Gaussian
	- $$f\sim GP(\mu, k)$$
	- where $$\mu(x)$$ and $$k(x, x^\prime)$$ are mean and covariance function.
	-