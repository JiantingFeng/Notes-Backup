- Gaussian process is equivalent to the bayesian learning with exact 1 infinite width hidden layer.
- Idea: **prior distribution over infinite dimensional functions**.
- Gaussian Process Regression
	- Gaussian distribution on one single point: $$P(y_i\vert x_i; w)=\mathcal{N}(w^Tx, \sigma^2 I)$$
	- MLE: $$\displaystyle P(\mathcal{D}\vert w) = \prod_{i=1}^n P(y_i\vert x_i; w)$$
	- MAP: $$P(w\vert \mathcal D) = \dfrac{P(\mathcal{D}\vert w) P(w)}{Z}$$
	- Directly model the distribution of $$y$$ (w/o learning the model)
		- $$P(y\vert x, \mathcal D) = \int_w \underbrace{P(y\vert x, w)}_{\text{Gaussian}} \underbrace{P(w\vert \mathcal D)}_{\text{Gaussian}}dw$$
		- Therefore, $$P(y\vert x, \mathcal D)$$ is still Gaussian, we don't need to calculate the exact form, because we know how Gaussian looks like.
		- $$P(y\vert x, \mathcal D) \sim \mathcal{N}(\mu, \sigma^2)$$
-