\documentclass{paper}
\input{mathfonts.tex}
\input{permeable.tex}


\addbibresource{ref.bib}

\title{Machine Learning Theory}
\author{FENG JIANTING}
\date{\today}
\begin{document}
\maketitle
This is a collection of summary of resources in theoretical machine learning.
\tableofcontents
\part{High Dimensional Statistics}
This part is mainly focus on preliminary knowledge for statistics in high dimensional space.
\section{Concentration Inequalities}
A simple way to control a tail probability $\PP{X}\geq t$ is by its moments. Higer-order moments always leads to a sharper bounds. A basic result is Markov's ineuqality
\[
	\PP \left[X\geq t\right] \leq \frac{\EE X}{t}, \quad \text{for all } t > 0.
\]
which requires finite mean $\EE{X}$. The proof is quiet intuitive,
\begin{proof}
	For all $t > 0$
	\begin{align}
		\PP\left[X\geq t\right] & = \int_{x\geq t} dP               \\
		                        & \leq \int_{x\geq t} \frac{x}{t}dP \\
		                        & \leq\int_{\RR}\frac{x}{t}dP       \\
		                        & = \frac{\EE X}{t}
	\end{align}
\end{proof}

Another similar result is Chebyshev's inequality
\[
	\PP{\left[\vert X-\mu\vert \geq t\right]} \leq \frac{\var{X}}{t^2}, \quad \text{for all } t > 0.
\]
where $\mu=\EE X$.

Similarly, for any $k\in \NN_+$, we always have
\[
	\PP\left[\vert X-\mu\vert\geq t\right] \leq \frac{\EE\left[\vert X-\mu\vert^k\right]}{t^k}
\]

once $\EE\left[\vert X-\mu\vert^k\right]$ exists.

Of course, other strictly increase functions can be applied to $\vert X-\mu\vert ^k$ other than just polynomials.
Consider the moment generating function $\varphi(\lambda) = \EE \left[e^{\lambda(X-\mu)}\right]$ exists for $\lambda \leq \vert b\vert$ where $b>0$ is some constant.
We can apply Markov's inequality to $Y = \exp(\lambda (X-\mu))$, thereby, we obtain
\[
	\PP\left[(X-\mu)\geq t\right] \PP\left[e^{\lambda(X-\mu)}\geq e^{\lambda t}\right] \leq \frac{\EE\left[e^{\lambda(X-\mu)}\right]}{e^{\lambda t}}
\]
take logarithm, and optimizing the RHS, which yields the \textit{Chernoff} bound

\[
	\log \PP\left[(X-\mu)\geq t\right] \leq \inf_{\lambda\in[0, b]}\left\{\log\EE\left[e^{\lambda(X-\mu)}\right] - \lambda t\right\}
\]
\subsection{Sub-Gaussian variables and Hoeffding bounds}
Base on the discussion above, it's easy to come up with the idea of classify the r.v. based on their MGF. here we give an example, which consider all the r.v. with tail "lower" than gaussian r.v.

\begin{exmp}
	Let $X\sim \mcN(\mu, \sigma^2)$ be a Gaussian r.v. with mean $\mu$ and variance $\sigma^2$. By straightforward calculation, we get
	\[
		\EE\left[e^{\lambda (X-\mu)}\right] = e^{\frac{\lambda^2\sigma^2}{2}}
	\]
	for all $\lambda \in \RR$. Then take $\inf$,
	\[
		\inf_{\lambda\geq 0}\left\{\log\EE\left[e^{\lambda(X-\mu)}\right]-\lambda t\right\} = \inf_{\lambda\geq 0}\left\{\frac{\sigma^2\lambda^2}{2}-\lambda t\right\} = -\frac{t^2}{2\sigma^2}
	\]
	The tail probability is bounded by
	\[
		\PP\left[X-\mu\geq t\right] \leq e^{-\frac{t^2}{2\sigma^2}}
	\]
\end{exmp}
Motivated by this, we can define
\begin{defn}
	A r.v. $X$ with $\mu=\EE X$ is sub-Gaussian if there exists a positive $\sigma>0$ such that
	\[
		\EE{\left[e^{\lambda(X-\mu)}\right]}	 \leq e^{\sigma^2\lambda^2/2}
	\]
	for all $\lambda \in\RR$.
\end{defn}
That is, the MGF of the given r.v. is bounded by the MGF of Gaussian r.v..The constant $\sigma$ is referred to as the sub-Gaussian parameter. Based on the definition, we can easily get the concentration inequality for sub-Gaussian r.v.
\[
	\PP\left[\vert X-\mu\vert \geq t\right] \leq 2e^{-\frac{t^2}{2\sigma^2}}
\]
for all $t\in\RR$.
\begin{exmp}
	A Rademacher r.v. $\varepsilon$ takes the value $\{-1, 1\}$ with the same probability. Indeed, it's a sub-Gaussian r.v. with parameter $\sigma=1$.
\end{exmp}
\begin{claim}
	Any r.v. supported finitely are sub-Gaussian.
\end{claim}
\begin{exmp}
	Let $X$ be zero-mean, and supported on some intervals $[a, b]$. Letting $X^\prime$ be an independent copy of  $X$, for any $\lambda\in\RR$, we have
	\[
		\EE_{X}\left[e^{\lambda X}\right] = \EE_{X} \left[e^{\lambda(X-\EE_{X^\prime}[X^\prime])}\right] \leq \EE_{X, X^\prime}\left[e^{\lambda(X-X^\prime)}\right]
	\]
	the inequality is given by the convexity of $e^{-\lambda x}$. Let $\varepsilon$ be Rademacher r.v., note that $\varepsilon(X-X^\prime)$ has the same distribution as $X-X^\prime$.
	So we have
	\[
		\EE_{X, X^\prime}\left[e^{\lambda(X-X^\prime)}\right] =
		\EE_{X, X^\prime}\left[\EE_{\varepsilon}\left[e^{\lambda\varepsilon(X-X^\prime)}\right]\right] \leq
		\EE_{X, X^\prime}\left[e^{\frac{\lambda^2(X-X^\prime)^2}{2}}\right]
	\]
	the inequality is given by the fact that Rademacher r.v. is sub-Gaussian. Since $\vert X-X^\prime\vert\leq b-a$, we have
	\[
		\EE_{X, X^\prime}\left[e^{\frac{\lambda^2(X-X^\prime)^2}{2}}\right] \leq e^{\frac{\lambda^2(b-a)^2}{2}}
	\]
	Which shows that bounded r.v. is sub-Gaussian with parameter $\sigma = \frac{b-a}{2}$.
\end{exmp}
Here we propose the general Hoeffding bound,
\begin{prop}
	Suppose that the variables $X_i, i=1,\cdots, n$ are independent, and $X_i$ has mean $\mu_i$ with sub-Gaussian parameter $\sigma_i$. Then for all $t\geq 0$, we have
	\[
		\PP\left[\sum_{i=1}^n (X_i-\mu_i)\geq t\right] \leq \exp{\left\{-\frac{t^2}{2\sum_{i=1}^n\sigma_i^2}\right\}}
	\]
\end{prop}
\begin{proof}
	\begin{align*}
		\PP\left[\sum_{i=1}^n (X_i-\mu_i)\geq t\right] & \leq \exp\left\{\frac{\lambda^2}{2}\sum_{i=1}^n\sigma_i^2 - \lambda t\right\} \\
		                                               & \leq \exp{\left\{-\frac{t^2}{2\sum_{i=1}^n\sigma_i^2}\right\}}
	\end{align*}
\end{proof}
Following theorem gives us the equivalent characterization of sub-Gaussian r.v.
\begin{thm}
	Given any zero-mean r.v., the following properties are equivalent:
	\begin{enumerate}
		\item There is a constant $\sigma\geq 0$ such that
		      \[
			      \EE\left[e^{\lambda X}\right] \leq e^{\frac{\lambda^2\sigma^2}{2}} \quad \text{for all } \lambda\in \RR
		      \]
		\item There is a constant $c\geq 0$ and Gaussian r.v. $Z\sim\mcN(0, \tau)$ such that
		      \[
			      \PP\left[\vert X\vert\geq s\right]\leq c\PP\left[\vert Z\vert \geq s\right], \quad \text{for all } s\geq 0
		      \]
		\item There is a constant $\theta\geq 0$ such that
		      \[
			      \EE\left[X^{2k}\right]\leq \frac{(2k)!}{2^k k!} \theta^{2k} \quad \text{for all } k=1, 2, \cdots
		      \]
		\item There is a constant $\sigma\geq 0$ such that
		      \[
			      \EE\left[e^{\frac{\lambda X^2}{2\sigma^2}}\right]\leq \frac{1}{\sqrt{1-\lambda}} \quad\text{for all } \lambda \in [0, 1)
		      \]
	\end{enumerate}
\end{thm}
\subsection{Sub-exponential variables and Bernstein bounds}
The notion of sub-Gaussianity is fairly restrictive, so it's natrual to relax it. Accordingly, we now turn to sub-exponential r.v. with milder condition.
\begin{defn}
	A random variable $X$ with $\mu=\EE X$ is sub-exponential if there are non-negative parameters $(\nu,\alpha)$ such that
	\[
		\EE\left[e^{\lambda(X-\mu)}\right]\leq e^{\frac{\nu^2 \lambda^2}{2}} \quad \text{for all } \vert \lambda\vert < \frac{1}{\alpha}.
	\]
\end{defn}
It immediately follows that all sub-Gaussian r.v. are sub-Exponential, take $(\nu, \alpha) = (\sigma, 0)$. However, the converse doesn't hold.
\begin{exmp}
	Let $Z\sim\mcN(0, 1)$, and consider the r.v. $X=Z^@$. For $\lambda<\frac{1}{2}$,
	\begin{align*}
		\EE\left[e^{\lambda(X-1)}\right] & = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{\lambda (z^2-1)e^{-z^2/2}} dz \\
		                                 & =\frac{e^{-\lambda}}{\sqrt{1-2\lambda}}
	\end{align*}
	for $\lambda>\frac{1}{2}$, the MGF doesn't exists. For all $\vert \lambda\vert<\frac{1}{4}$,
	\[
		\frac{e^{-\lambda}}{\sqrt{1-2\lambda}} \leq e^{2\lambda^2} = e^{4\lambda^2/2}
	\]
	therefore, $Z$ is a $(\nu, \alpha) = (4, 2)$ sub-exponential r.v.
\end{exmp}
\begin{prop}
	Suppose that $X$ is sub-exponential with parameters $(\nu,\alpha)$. Then
	\[
		\PP\left[X-\mu \geq t\right] \leq
		\begin{cases}
			e^{-\frac{t^2}{2\nu^2}} & \text{if } 0 \leq t \leq \frac{\nu^2}{\alpha}, \\
			e^{-\frac{t}{2\alpha}}  & \text{if } t > \frac{\nu^2}{\alpha}.
		\end{cases}
	\]
\end{prop}
\begin{proof}
	Follow the Chernoff bound, we have
	\[
		\PP\left[X-\mu\geq t\right] \leq \exp\left(\log\EE[e^{\lambda X}] - \lambda t\right) \leq \exp\left( \frac{\nu^2\lambda^2}{2} - \lambda t\right)
	\]
	for $\lambda< \frac{1}{\alpha}$. Take $g(\lambda, t) = \frac{\nu^2\lambda^2}{2}-\lambda t$, consider the unconstrained optimization, the function takes minimum at $\lambda^*= \frac{t}{\nu^2}$.

	If $\frac{t}{\nu^2} < \frac{1}{\alpha}$, then $\displaystyle\inf_{\lambda\in[0,\alpha^{-1})}g(\lambda, t) = -\frac{t^2}{2\nu^2}$.

	Otherwise, if $t \geq \frac{\nu^2}{\alpha}$, the minimum achieves at $\lambda^*=\alpha^{-1}$, and
	\[
		\inf_{\lambda\in[0,\alpha^{-1})} g(\lambda, t) = -\frac{t}{\alpha^2} + \frac{\nu^2}{2\alpha^2}\leq -\frac{t}{2\alpha}
	\]
	the inequality is derived from $\frac{\nu^2}{\alpha}\leq t$.
\end{proof}
Based on the example of $X^2$, tsub-exponential r.v. can be verified explicitly with boundingthe MGF, however, in many settings, it's impracticable.
An alternative approach is based on the control of polynomial moments, called \textit{Bernstein's condition}. Given a r.v. with $\mu=\EE X$ and $\sigma^2=\var{X}$, we say Bernstein's condition with parameter $b$ holds if
\[
	\vert\EE\left[(X-\mu)^k\right]\vert\leq \frac{1}{2}k!\sigma^2b^{k-2} \quad \text{for } k \geq 2.
\]
For any bounded r.v., the Bernstein condition is obvious.\par
When this condition holds, we can expand the MGF
\begin{align*}
	\EE\left[e^{\lambda (X-\mu)}\right] & = 1  +\frac{\lambda^2\sigma^2}{2} + \sum_{k=3}^\infty \lambda^k \frac{\EE\left[(X-\mu)^k\right]}{k!}            \\
	                                    & \leq 1+ \frac{\lambda^2\sigma^2}{2} + \frac{\lambda^2\sigma^2}{2}\sum_{k=3}^\infty(\vert \lambda\vert b)^{k-2},
\end{align*}
Sum the geometric series so as to obtain
\[
	\EE\left[e^{\lambda (X-\mu)}\right] \leq 1 + \frac{\lambda^2\sigma^2/2}{1-b\vert\lambda\vert} \leq e^{\frac{\lambda^2\sigma^2/2}{1-b\vert\lambda\vert}}
\]
Consequently, we conclude that
\[
	\EE\left[e^{\lambda (X-\mu)}\right]\leq e^{\frac{\lambda^2(\sqrt{2}\sigma)^2}{2}} \quad \text{for all } \vert \lambda\vert <\frac{1}{2b}.
\]
which is a sub-exponential r.v. with $(\nu, \alpha) = (\sqrt{2}\sigma, 2b)$.
\begin{prop}
	For any r.v. satisfying Bernstein condition, we have
	\[
		\EE\left[e^{\lambda (X-\mu)}\right] \leq e^{\frac{\lambda^2\sigma^2/2}{1-b\vert\lambda\vert}}
	\]
	for all $\vert\lambda\vert < \frac{1}{b}$. Moveover,
	\[
		\PP\left[\vert X-\mu\vert\geq t\right] \leq 2e^{\frac{t^2}{2(\sigma^2+bt)}} \quad \text{for all } t \geq 0
	\]
\end{prop}
Take $\lambda = \frac{t}{bt+\sigma^2}\in [0, b^{-1})$ in Chernoff bound, the tail bound can be derived.

Similar to sub-Gaussian r.v., the sum of sub-exponential r.v. is also a sub-exponential r.v. with the following bound, take $\{X_k\}_{k=1}^n$ with
\[
	\alpha_* =\max_{k\in[n]}\alpha_k\quad \text{and}\quad \nu_* = \sqrt{\sum_{k=1}^{n}v_k^2}.
\]
then
\[
	\PP\left[\frac{1}{n}\sum_{i=1}^{n}(X_k-\mu_k)\geq t\right] \leq \begin{cases}
		e^{-\frac{nt^2}{2(\nu_*^2/n)}}, & \text{ for } 0\leq t\leq \frac{\nu_*^2}{n\alpha_*}, \\
		e^{-\frac{nt}{2\alpha_*}},      & \text{ for } t > \frac{\nu^2}{n\alpha_*}
	\end{cases}
\]
\begin{exmp}
	A chi-squared ($\chi^2$) r.v. with $n$ degrees of freedom, denoted by $Y\sim\chi_n^2$, can be represented as the sum
	\[
		Y = \sum_{k=1}^{n}Z_k^2, \text{ where } Z_k\sim \mcN(0, 1)
	\]
	as preceding discussion, $Z_k$ is $(2, 4)$ sub-exponential r.v., therefore, $Y$ is $(2\sqrt{n}, 4)$ sub-exponential, we can easily deduced that
	\[
		\PP\left[\left\vert \frac{1}{n}\sum_{k=1}^{n}Z_k^2-1\right\vert\geq 1\right] \leq 2e^{-nt^2/8}, \quad \text{for all } t\in(0, 1)
	\]
\end{exmp}
The concentration of $\chi^2$ r.v. takes an important role in the following Johnson-Lindenstrauss theorem for random projections.
\begin{exmp}
	Suppose given $N\geq 2$ distinct vectors $\{u^1, \cdots, u^N\}\subset\RR^d$, if the data dimension $d$ is large, then it might be expensive to store and manipulate the data set.
	The idea of dimensionality reduction is to construct a mapping $F:\RR^d\to\RR^m$ with $m\ll d$, which guarantee that
	\[
		1-\delta \leq \frac{\lVert F(u^i)-F(u^j)\rVert_2^2}{\lVert u^i-u^j\rVert_2^2} \leq 1+\delta, \quad \text{ for all pairs } u^i\neq u^j.
	\]
	The construction is probabilistic: we implement random projection with a random matrix $\bmX\in\RR^{m\times d}$ filled with independent $\mcN(0, 1)$ elements. And define
	\begin{align*}
		F: \RR^d & \to \RR^m               \\
		u        & \mapsto \bmX u/\sqrt{m}
	\end{align*}
	Now, we verify that $F$ satisfies the given condition with high probability. Let $x_i\in \RR^d$ denote the i-th row of $\bmX$, fix $u\in\RR^d$,
	we know that $\langle x_i, \frac{u}{\lVert u\rVert_2}\rangle \sim \mcN(0, 1)$. Therefore,
	\[
		Y = \frac{\lVert \bmX u\rVert_2^2}{\lVert u\rVert_2^2} = \sum_{i=1}^{m}\langle x_i, u/\lVert u\rVert\rangle^2,
	\]
	follows a $\chi_m^2$ r.v., where the d.f. is $m$. Therefore, applying the previous bound for $\chi^2$ distribution,
	\[
		\PP\left[\left\vert\frac{\lVert\bmX u\rVert_2^2}{m\lVert u\rVert_2^2} - 1\right\vert\geq \delta\right] \leq 2e^{-m\delta^2/8} \quad \text{ for all } \delta \in (0, 1).
	\]
	Rearranging and recalling the definition of $F$ yields that
	\[
		\PP\left[\frac{\lVert F(u)\rVert_2^2}{\lvert u\rVert_2^2}\notin[1-\delta, 1+\delta]\right]\leq 2e^{-m\delta^2/8} \quad \text{ for any fixed } 0\neq u\in\RR^d.
	\]
	Note that there exists $\binom{N}{2}$ pairs of distinct $u$, we apply the union bound and get
	\[
		\PP\left[\frac{\lVert F(u^i-u^j)\rVert_2^2}{\lVert u^i-u^j\rVert_2^2}\notin[1-\delta, 1+\delta]\text{ for some } u^i\neq u^j\right]\leq 2\binom{N}{2}e^{-m\delta^2/8}
	\]
	for any $\epsilon\in(0, 1)$, this probability can be bound below $\epsilon$ by choosing $m\geq \frac{C}{\delta^2}\log N$.
\end{exmp}
\begin{thm}
	For a zero-mean r.v. $X$, the following statements are equivalent:
	\begin{enumerate}
		\item There are non-negative numbers $\nu, \alpha$ such that
		      \[
			      \EE[e^{\lambda X}]\leq e^{\frac{\nu^2\lambda^2}{2}}\quad \text{ for all } \vert \lambda\vert < \frac{1}{\alpha}
		      \]
		\item There is a positive number $c_0$ such that $\EE[e^{\lambda X}]<\infty$ for all $\lambda<c_0$
		\item There are constants $c_1, c_2>0$ such that
		      \[
			      \PP[\vert X\vert\geq t] \leq c_1e^{-c_2t} \quad \text{ for all } t>0.
		      \]
		\item The quantity $\gamma=\sup_{k\geq 2}\left[\frac{\EE[X^k]}{k!}\right]^{1/k}$ is finite.
	\end{enumerate}
\end{thm}
One condition of two-sided Bernstein bound holds is $\vert X\vert \leq b$ almost surely, but if we only have one-sided inequality, we can still get some one-sided bounds.

\printbibliography
\end{document}
