@misc{dongAttentionNotAll2021,
  title = {Attention Is {{Not All You Need}}: {{Pure Attention Loses Rank Doubly Exponentially}} with {{Depth}}},
  shorttitle = {Attention Is {{Not All You Need}}},
  author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  year = {2021},
  month = mar,
  number = {arXiv:2103.03404},
  eprint = {2103.03404},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/fengjianting/Zotero/storage/JXW25RWY/Dong et al. - 2021 - Attention is Not All You Need Pure Attention Lose.pdf;/Users/fengjianting/Zotero/storage/UCI8VKGZ/2103.html}
}

@misc{kengProbabilisticInterpretationRegularization2016,
  title = {A {{Probabilistic Interpretation}} of {{Regularization}}},
  author = {Keng, Brian},
  year = {2016},
  month = aug,
  journal = {Bounded Rationality},
  urldate = {2023-04-16},
  abstract = {A look at regularization through the lens of probability.},
  howpublished = {http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/},
  langid = {english},
  file = {/Users/fengjianting/Zotero/storage/R9G3A2PF/probabilistic-interpretation-of-regularization.html}
}
